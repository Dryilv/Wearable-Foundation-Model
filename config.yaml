train:
  # 2w 小时数据量巨大，200 epoch 可能需要训练很久。
  # 建议先跑 50-100 epoch 看 Loss 收敛曲线。
  epochs: 100             
  
  # [显存优化] 
  # 引入张量分解后，显存占用会降低。
  # 24G 显存建议: 64 - 96
  # 80G 显存建议: 256 - 512
  batch_size: 256          
  
  base_lr: 1.5e-4         # 稍微调大一点，因为 Batch Size 较大且数据量大
  min_lr: 1.0e-6
  warmup_epochs: 5        # 数据量大，Warmup 可以稍微短一点
  weight_decay: 0.05      # 张量分解本身有正则化效果，WD 可以适当降低 (0.1 -> 0.05)
  clip_grad: 3.0          
  use_amp: True           # 必须开启，配合代码中的 bfloat16
  
  log_interval: 50
  save_freq: 5
  
  # 修改保存路径，区分新旧模型
  save_dir: "./checkpoint_rope_tensor_768"
  resume: ""              # 如果是第一次跑新模型，置空；否则填路径

data:
  index_path: "/home/bml/storage/mnt/v-044d0fb740b04ad3/org/WFM/model/SharedPhysioTFMAE/train_index.json"
  signal_len: 3000        # 30秒 @ 100Hz
  num_workers: 16         # 保持高并发加载

model:
  # --- CWT & Patch 参数 ---
  cwt_scales: 64          
  
  # [关键] 深度可分离 PatchEmbed 配置
  # 时间轴: 3000 / 50 = 60 patches
  # 频率轴: 64 / 4 = 16 patches
  # 总 Token = 960
  patch_size_time: 50     
  patch_size_freq: 4      
  
  # --- Transformer 架构 (ViT-Base) ---
  embed_dim: 768          
  depth: 12               
  num_heads: 12           
  
  # --- [新增] 张量分解参数 ---
  # 控制 MLP 层的秩。
  # 0.25: 极度压缩，适合小数据防过拟合。
  # 0.50: 平衡点，适合中大数据 (推荐)。
  # 0.75-1.0: 高容量，适合超大规模数据。
  mlp_rank_ratio: 0.5     
  
  # --- Decoder ---
  decoder_embed_dim: 512  
  decoder_depth: 8        
  decoder_num_heads: 16   
  
  # --- 训练策略 ---
  # [重要] 针对 ECG/PPG 的高自相关性，必须高 Mask Ratio
  # 0.75 意味着遮住 22.5秒，只留 7.5秒让模型猜。
  mask_ratio: 0.75         
  
  # --- Loss 权重 ---
  # 建议提高时域 Loss 权重，确保重建出的波形 R 波尖锐，无相位偏差
  time_loss_weight: 1.0