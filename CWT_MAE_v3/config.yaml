train:
  epochs: 100             
  
  # [显存优化] 3000点信号较长，物理 Batch Size 建议设为 16-32 (视显存而定)
  batch_size: 32        
  
  # [关键修改] 梯度累积步数
  # 有效 Batch Size = 32 * 8 = 256，保证 MAE 预训练的稳定性
  accum_iter: 8
  
  base_lr: 1.5e-4         
  min_lr: 1.0e-6
  warmup_epochs: 5        
  auto_scale_lr: True     
  
  weight_decay: 0.05      
  clip_grad: 3.0
  
  use_amp: True           
  
  log_interval: 50
  save_freq: 5
  save_dir: "./checkpoint_768_5channel_3000pts"
  
  eval_freq: 20           
  linear_probe_limit: 100 
  resume: ""

data:
  index_path: "train_index_cleaned.json"
  # [关键修改] 统一对齐到 3000 点
  signal_len: 3000        
  num_workers: 16         
  use_sliding_window: False 
  window_stride: 1500      # 如果未来开启滑动窗口，步长设为长度的一半

model:
  cwt_scales: 64          
  
  # [关键修改] 避免 Token 爆炸
  # Time=30, Freq=8 -> Tokens = (3000/30) * (64/8) = 100 * 8 = 800 tokens/channel
  # 既保留了 0.3s 的精细时间分辨率，又将计算量控制在合理范围
  patch_size_time: 30     
  patch_size_freq: 8      
  use_conv_stem: True     
  
  embed_dim: 768          
  depth: 12               
  num_heads: 12           
  
  mlp_rank_ratio: 0.5     
  
  decoder_embed_dim: 512  
  decoder_depth: 8        
  decoder_num_heads: 16   
  
  # 配合 Tubelet Masking，0.6 是一个非常好的起点
  mask_ratio: 0.6         
  
  # 新版代码中时域 Loss 仅计算 Mask 部分，数值会变小，0.1 可作为起点，视 wandb 曲线调整
  time_loss_weight: 0.1
  
  data_ratio: 1.0