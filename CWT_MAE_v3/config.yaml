# ==============================================================================
# CWT-MAE Training Configuration (Optimized for 80GB GPU)
# ==============================================================================

# ------------------------------------------------------------------------------
# 1. Training & Optimization
# ------------------------------------------------------------------------------
train:
  epochs: 200               # [建议] MAE 通常需要较长训练周期，100 可能未收敛，建议 200-400
  
  # [显存优化] 80GB 显卡火力全开
  # 单塔模式下内部 Batch 会翻倍，但 256 依然很轻松。
  # 如果显存不够，降到 128；如果显存还有剩，大胆开到 512。
  batch_size: 256            
  
  accum_iter: 1             

  # [学习率调整] 配合较大的 Batch Size
  base_lr: 1.5e-4           # 配合 auto_scale_lr，实际 lr ≈ 1.5e-4 * 256 / 256 = 1.5e-4
  min_lr: 1.0e-6            
  warmup_epochs: 10         # [建议] 增加预热轮数，保证对比学习初期稳定
  auto_scale_lr: True       

  weight_decay: 0.05        
  clip_grad: 3.0            
  use_amp: True             

  log_interval: 50          
  save_freq: 10             # 减少保存频率，节省硬盘
  save_dir: "./checkpoint_vit_base_contrastive" 
  resume: ""                

# ------------------------------------------------------------------------------
# 2. Dataset Configuration
# ------------------------------------------------------------------------------
data:
  index_path: "train_index_cleaned.json"
  signal_len: 3000          
  num_workers: 16           
  use_sliding_window: False 
  window_stride: 1500       

# ------------------------------------------------------------------------------
# 3. Model Architecture (升级为 ViT-Base 规格)
# ------------------------------------------------------------------------------
model:
  # --- Input Processing ---
  cwt_scales: 64            
  use_conv_stem: True       
  
  # 保持你优秀的 Token 计算逻辑 (800 Tokens)
  patch_size_time: 30       
  patch_size_freq: 8        

  # --- Transformer Encoder (升级至 ViT-Base) ---
  # 256 -> 768: 提升特征表达能力，这对跨模态对齐至关重要
  embed_dim: 768            
  depth: 12                 
  num_heads: 12             # 768 / 64 = 12 heads
  mlp_rank_ratio: 4.0       # [标准] ViT 标准 MLP ratio 是 4.0，0.5 太小了！

  # --- Transformer Decoder ---
  decoder_embed_dim: 512    # 配合 Encoder 升级
  decoder_depth: 8          
  decoder_num_heads: 16     # 512 / 32 = 16 heads

  # --- Masking Strategy ---
  mask_ratio: 0.75          

  # --- Signal Difference ---
  use_diff: True
  diff_loss_weight: [1.0, 0.5, 0.1]

  # --- Loss Weighting ---
  time_loss_weight: 1.0     # [建议] 稍微提高时域权重，因为加入了残差连接，时域重建更重要了
  
  # --- Contrastive Learning ---
  proj_dim: 256             
  temperature: 0.07         
  contrastive_weight: 1.0