import torch
import numpy as np
import os
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

# å¯¼å…¥çº¯å‡€ç‰ˆ PatchTST
from patchtst_model import PatchTST_Pretrain

# ==========================================
# 1. é…ç½®å‚æ•° (Configurations)
# ==========================================
DATA_PATH = "real_ecg_ppg_data.npy"  # æ›¿æ¢ä¸ºä½ çš„.npyçœŸå®æ•°æ®æ–‡ä»¶å
SAVE_DIR = "checkpoints_patchtst"  # æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„

SEQ_LEN = 512
PATCH_LEN = 8  # åˆ‡å—å¤§å°
STRIDE = 8  # æ­¥é•¿ (ä¸ä½ çš„ CWT-MAE ä¿æŒä¸€è‡´)
IN_CHANNELS = 2  # ECG å’Œ PPG ä¸¤ä¸ªé€šé“

# è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE = 128  # ç‰©ç† Batch Size
ACCUMULATION_STEPS = 16  # æ¢¯åº¦ç´¯ç§¯ï¼Œé€»è¾‘ Batch Size = 128 * 16 = 2048
LR = 1e-4
EPOCHS = 10
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ==========================================
# 2. æé€Ÿå†…å­˜æ˜ å°„ Dataset
# ==========================================
class PatchTSTDataset(Dataset):
    def __init__(self, data_path):
        super().__init__()
        # ä½¿ç”¨ mmap_mode='r'ï¼Œ20GB æ•°æ®ç¬é—´åŠ è½½ï¼Œä¸å å†…å­˜
        self.data = np.load(data_path, mmap_mode='r')
        self.total_samples = self.data.shape[0]
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œæ€»æ ·æœ¬æ•°: {self.total_samples}")

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        # è¯»å–å•æ¡æ•°æ®ï¼Œå½¢çŠ¶ä¸º (2, 512) -> (Channels, Seq_Len)
        sample = self.data[idx].copy()  # copy é˜²æ­¢ä¿®æ”¹åªè¯»çš„ mmap

        # 1. ç‹¬ç«‹é€šé“å½’ä¸€åŒ– (Instance Normalization / RevIN é£æ ¼)
        # PatchTST å¯¹æ•°æ®çš„å°ºåº¦éå¸¸æ•æ„Ÿï¼Œå¿…é¡»åœ¨åˆ‡å—å‰è¿›è¡Œ Z-score å½’ä¸€åŒ–
        mean = sample.mean(axis=1, keepdims=True)
        std = sample.std(axis=1, keepdims=True)
        std = np.clip(std, a_min=1e-5, a_max=None)
        sample = (sample - mean) / std

        # 2. è½¬æ¢å½¢çŠ¶ä»¥é€‚é… PatchTST æ¨¡å‹
        # PatchTST è¦æ±‚çš„è¾“å…¥æ˜¯ [Seq_Len, Channels]ï¼Œæ‰€ä»¥éœ€è¦è½¬ç½®
        sample = np.transpose(sample, (1, 0))  # å˜æˆ (512, 2)

        return torch.tensor(sample, dtype=torch.float32)


# ==========================================
# 3. æ ¸å¿ƒè®­ç»ƒå¾ªç¯ (Training Loop)
# ==========================================
def train():
    os.makedirs(SAVE_DIR, exist_ok=True)

    # åˆå§‹åŒ– Dataset å’Œ DataLoader
    dataset = PatchTSTDataset(DATA_PATH)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,  # å¼€å¯å¤šè¿›ç¨‹åŠ é€Ÿè¯»å–
        pin_memory=True
    )

    # åˆå§‹åŒ–æ¨¡å‹ (å‚æ•°é‡ä¸ä½ çš„ CWT-MAE å¯¹é½)
    model = PatchTST_Pretrain(
        seq_len=SEQ_LEN,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        in_channels=IN_CHANNELS,
        d_model=768,  # éšè—å±‚ç»´åº¦
        n_heads=12,  # æ³¨æ„åŠ›å¤´æ•°
        e_layers=12,  # Transformer å±‚æ•°
        mask_ratio=0.75  # æ©ä½ 75% çš„æ•°æ®è®©å®ƒçŒœ
    ).to(DEVICE)

    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    scaler = GradScaler()  # æ··åˆç²¾åº¦åŠ é€Ÿå™¨

    print(f"ğŸš€ å¼€å§‹ PatchTST é¢„è®­ç»ƒ (è®¾å¤‡: {DEVICE})...")

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0
        optimizer.zero_grad()

        # ä½¿ç”¨ tqdm æ‰“å°è¿›åº¦æ¡
        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch + 1}/{EPOCHS}")

        for step, x in pbar:
            x = x.to(DEVICE)  # x shape: [Batch, 512, 2]

            # å¼€å¯è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)ï¼Œæ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦ç¿»å€
            with autocast():
                # å‰å‘ä¼ æ’­ï¼ŒPatchTST_Pretrain è¿”å›çš„æ˜¯ (loss, é¢„æµ‹patch, çœŸå®patch, mask)
                loss, _, _, _ = model(x)

                # æ¢¯åº¦ç´¯ç§¯ï¼šå¯¹ loss è¿›è¡Œç¼©æ”¾
                loss = loss / ACCUMULATION_STEPS

            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()

            # å½“ç´¯ç§¯åˆ°æŒ‡å®šçš„æ­¥æ•°æ—¶ï¼Œæ›´æ–°ä¸€æ¬¡æƒé‡
            if (step + 1) % ACCUMULATION_STEPS == 0 or (step + 1) == len(dataloader):
                scaler.unscale_(optimizer)
                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢ Transformer è®­ç»ƒåˆæœŸæ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            # è®°å½• Loss (ç”±äºä¹‹å‰é™¤äº† ACCUMULATION_STEPSï¼Œè¿™é‡Œä¹˜å›æ¥ä»¥ä¾¿æ˜¾ç¤ºçœŸå®çš„ Loss)
            current_loss = loss.item() * ACCUMULATION_STEPS
            total_loss += current_loss

            # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„ Loss æ˜¾ç¤º
            pbar.set_postfix({"Loss": f"{current_loss:.4f}"})

        # æ‰“å° Epoch ç»Ÿè®¡ä¿¡æ¯
        avg_loss = total_loss / len(dataloader)
        print(f"âœ… Epoch [{epoch + 1}/{EPOCHS}] Average Loss: {avg_loss:.4f}")

        # ä¿å­˜ Checkpoint
        save_path = os.path.join(SAVE_DIR, f"patchtst_pretrain_epoch_{epoch + 1}.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
        }, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {save_path}\n")


if __name__ == "__main__":
    # æé†’ï¼šè¿è¡Œå‰ç¡®ä¿ä½ çš„ real_ecg_ppg_data.npy è·¯å¾„æ­£ç¡®
    train()